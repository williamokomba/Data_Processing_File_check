{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How you can reduce the dataset size and speedup processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import depedancies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#create a function for generating fake data\n",
    "def get_dataset(size):\n",
    "    # Create Fake Dataset\n",
    "    df = pd.DataFrame()\n",
    "    df['size'] = np.random.choice(['big','medium','small'], size)\n",
    "    df['age'] = np.random.randint(1, 50, size)\n",
    "    df['team'] = np.random.choice(['red','blue','yellow','green'], size)\n",
    "    df['win'] = np.random.choice(['yes','no'], size)\n",
    "    dates = pd.date_range('2020-01-01', '2022-12-31')\n",
    "    df['date'] = np.random.choice(dates, size)\n",
    "    df['prob'] = np.random.uniform(0, 1, size)\n",
    "    return df\n",
    "\n",
    "def set_dtypes(df):\n",
    "    df['size'] = df['size'].astype('category')\n",
    "    df['team'] = df['team'].astype('category')\n",
    "    df['age'] = df['age'].astype('int16')\n",
    "    df['win'] = df['win'].map({'yes':True, 'no': False})\n",
    "    df['prob'] = df['prob'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing CSV\n",
      "CPU times: total: 12.7 s\n",
      "Wall time: 16.4 s\n",
      "CPU times: total: 1.98 s\n",
      "Wall time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing CSV')\n",
    "df = get_dataset(5_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_csv('test.csv', index= False)\n",
    "%time df_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is New Volume\n",
      " Volume Serial Number is B8F9-3D09\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "11/08/2024  23:32       210,559,519 test.csv\n",
      "               1 File(s)    210,559,519 bytes\n",
      "               0 Dir(s)  11,781,398,528 bytes free\n"
     ]
    }
   ],
   "source": [
    "#check the csv size\n",
    "%ls -G Flash test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Pickle\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 141 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 77.7 ms\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing Pickle')\n",
    "df = get_dataset(5_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_pickle('test.pickle')\n",
    "%time df_pickle = pd.read_pickle('test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is New Volume\n",
      " Volume Serial Number is B8F9-3D09\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "11/08/2024  23:33        85,001,729 test.pickle\n",
      "               1 File(s)     85,001,729 bytes\n",
      "               0 Dir(s)  11,781,402,624 bytes free\n"
     ]
    }
   ],
   "source": [
    "#check pickle file size\n",
    "%ls -G Flash test.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serializing using Parquet\n",
    "#install parquet using either of the below\n",
    "#%pip install pyarrow\n",
    "#or\n",
    "#%pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Parquet\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.13 s\n",
      "CPU times: total: 594 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing Parquet')\n",
    "df = get_dataset(5_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_parquet('test.parquet')\n",
    "%time df_parquet = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is New Volume\n",
      " Volume Serial Number is B8F9-3D09\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "11/08/2024  23:33        36,960,385 test.parquet\n",
      "               1 File(s)     36,960,385 bytes\n",
      "               0 Dir(s)  11,781,402,624 bytes free\n"
     ]
    }
   ],
   "source": [
    "#test the size\n",
    "%ls -G Flash test.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Feather\n",
      "CPU times: total: 125 ms\n",
      "Wall time: 570 ms\n",
      "CPU times: total: 125 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "#Serializing in feather\n",
    "print('Reading and writing Feather')\n",
    "df = get_dataset(5_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_feather('test.feather')\n",
    "%time df_feather = pd.read_feather('test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is New Volume\n",
      " Volume Serial Number is B8F9-3D09\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "\n",
      " Directory of e:\\DS-work\\data_processes\n",
      "\n",
      "11/08/2024  23:33        51,267,490 test.feather\n",
      "               1 File(s)     51,267,490 bytes\n",
      "               0 Dir(s)  11,781,402,624 bytes free\n"
     ]
    }
   ],
   "source": [
    "#time it takes\n",
    "%ls -G Flash test.feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet seems is the best in terms of reducing the dataset\n",
    "\n",
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
